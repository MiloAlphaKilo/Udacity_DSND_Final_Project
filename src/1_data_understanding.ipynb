{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pycharm-5a24c1e6",
   "language": "python",
   "display_name": "PyCharm (udacity)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Data Analysis\n",
    "\n",
    "#### This notebook will be used for the analysis part of the project. Here we will explore and visualise the data to get a firm grasp on what the data looks like.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "source": [
    "## Import the datasets \n",
    "\n",
    "Two sets of data will be used in this project. \n",
    "\n",
    "1. The set of organisations fulfilled by the [IPEDS Data base](https://nces.ed.gov/ipeds/)\n",
    "2. The customer behavior is a ficticious data set creating random numbers and ranges. Fingers crossed we have some correlation in the data. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Create sqlite dB engine and read the data to frames"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure sqlite db engine\n",
    "engine = create_engine('sqlite:///../data/orgs_customer_behaviours.db')\n",
    "\n",
    "# Read data to frame\n",
    "\n",
    "# Import IPDES data\n",
    "# The data comes in the form of csv\n",
    "df_organisations_raw = pd.read_csv(\"../data/hd2019_convert.csv\", encoding='latin1') #encoding had to be added to be able to read the file\n",
    "# Import the synthetic customer behaviour data\n",
    "df_synth_customer_behaviour_data = pd.read_sql_table('synth_customer_behaviour_data', engine)\n",
    "df_synth_customer_target_classifier_data = pd.read_sql_table('synth_customer_target_classifier_data', engine)\n",
    "\n",
    "df_combined_customer_data = df_synth_customer_behaviour_data.merge(df_synth_customer_target_classifier_data,\n",
    "                             left_index=True, right_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data shape \n",
    "\n",
    "raw_org_columns = df_organisations_raw.shape[1]\n",
    "raw_org_rows = df_organisations_raw.shape[0]\n",
    "\n",
    "# Manual identification of columns that will have values \n",
    "columns_of_value = ['UNITID', 'INSTNM', 'IALIAS', 'CITY', 'STABBR', 'FIPS', 'OBEREG','GENTELE', 'EIN', 'DUNS', 'OPEID', \n",
    "                    'OPEFLAG', 'SECTOR', 'ICLEVEL', 'CONTROL', 'HLOFFER', 'GROFFER', 'HDEGOFR1', 'DEGGRANT', \n",
    "                    'HBCU', 'HOSPITAL', 'MEDICAL', 'TRIBAL', 'LOCALE', 'ACT', 'DEATHYR', 'CYACTIVE', \n",
    "                    'POSTSEC', 'PSEFLAG', 'PSET4FLG', 'RPTMTH', 'INSTCAT', 'C18BASIC', 'C18IPUG', 'C18IPGRD', 'C18UGPRF', \n",
    "                    'C18ENPRF', 'C18SZSET', 'C15BASIC', 'CCBASIC', 'CARNEGIE', 'LANDGRNT', 'INSTSIZE', 'F1SYSTYP', \n",
    "                    'F1SYSCOD', 'COUNTYNM', 'CNGDSTCD', 'CONVERTED']\n",
    "\n",
    "df_organisations = df_organisations_raw[columns_of_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataset to remove records that arent relevant\n",
    "# Remove institutions that have closed down  or removed from IPEDS using column CLOSEDAT & DEATHYR\n",
    "\n",
    "df_organisations_filtered = df_organisations.loc[(df_organisations['DEATHYR']==-2)]"
   ]
  },
  {
   "source": [
    "## Uderstanding the data\n",
    "* Get the shape of the data\n",
    "* Identify columns with categorical values\n",
    "* Visualise the organisations by state"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_columns = df_organisations_raw.shape[1]\n",
    "raw_rows = df_organisations_raw.shape[0]\n",
    "org_columns = df_organisations.shape[1]\n",
    "org_rows = df_organisations_raw.shape[0]\n",
    "fil_org_columns = df_organisations_filtered.shape[1]\n",
    "fil_org_rows = df_organisations_filtered.shape[0]\n",
    "is_nulls_counts = len(df_organisations_filtered.columns[df_organisations_filtered.isnull().sum()==0])\n",
    "categorical_vals = df_organisations_filtered.select_dtypes(include=['object'])\n",
    "categorical_vals_columns = list(categorical_vals.columns)\n",
    "\n",
    "print(f'The raw data has {raw_columns} columns and {raw_rows} rows')\n",
    "print(f'The unfiltered reduced data has {org_columns} columns and {org_rows} rows')\n",
    "print(f'The filtered organisations df has {fil_org_columns} columns and {fil_org_rows} rows')\n",
    "print(f'The df has {df_organisations_filtered.shape[1]} columns and {is_nulls_counts} columns without nulls')\n",
    "print(f'There are {categorical_vals.shape[1]} columns with categorical values, they are in the columns: '\n",
    "      f'\\n\\n{categorical_vals_columns}')\n"
   ]
  },
  {
   "source": [
    "### The distribution of organisations across states\n",
    "The Top 5 States with organisations: \n",
    "CA, NY, FL, PA, TX"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_organisations_filtered['STABBR'].hist(bins=55, grid=False, figsize=(40, 15), xlabelsize=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cust_raw_columns = df_organisations_filtered.shape[1]\n",
    "cust_raw_rows = df_organisations_filtered.shape[0]\n",
    "cust_is_nulls_counts = len(df_organisations_filtered.columns[df_organisations_filtered.isnull().sum()==0])\n",
    "cust_categorical_vals = df_organisations_filtered.select_dtypes(include=['object'])\n",
    "cust_categorical_vals_columns = list(cust_categorical_vals.columns)\n",
    "\n",
    "print(f'The raw customer data has {cust_raw_columns} columns and {cust_raw_rows} rows')\n",
    "print(f'The customer df has {cust_is_nulls_counts} columns without null values')\n",
    "print(f'There are {cust_categorical_vals.shape[1]} categorical values in the customer data set. They are in the columns: '\n",
    "      f'\\n\\n{cust_categorical_vals_columns}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "source": [
    "### Clean the datasets\n",
    "\n",
    "#### Replace `-2` and `-1` values in the data set\n",
    "The IPEDS database uses `-2` and `-1` to indicate `null` or empty values, this might cause issues when applying the segmentation logic. \n",
    "\n",
    "At this point I will replace all `[-2, -1]` values with `0`. The data set uses `[-2, -1]` in the context of `integer` and `string` values so both need to be accomodated "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_organisations_data = df_organisations_filtered.replace([-2, '-2', -1, '-1'], 0)"
   ]
  },
  {
   "source": [
    "### Save data to sql lite dB for further processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_organisations_data.to_sql('customers_with_behaviours', engine, if_exists='replace', index=False)\n",
    "df_organisations_filtered.to_sql('orgs_organisations', engine, if_exists='replace', index=False)\n",
    "df_combined_customer_data.to_sql('customer_behaviour_data', engine, if_exists='replace', index=False)"
   ]
  },
  {
   "source": [
    "## Check correlations in dataset\n",
    "\n",
    "I'll use the seaborn visualisations to illustrate the correlations "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.5)\n",
    "plt.subplots(figsize=(60,45))\n",
    "sns.heatmap(df_organisations_data.corr(), annot=True, linewidths=2, cmap=\"YlGnBu\", fmt=\".2f\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.5)\n",
    "plt.subplots(figsize=(20,10))\n",
    "sns.heatmap(df_combined_customer_data.corr(), annot=True, linewidths=.1, cmap=\"YlGnBu\", fmt=\".2f\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}