{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Data preparation\n",
    "\n",
    "Before the data can be sent through the KNN model it needs to be tidied. \n",
    "Here I will: \n",
    "\n",
    "* Remove nulls;\n",
    "* Create dummies as required;\n",
    "* Normalise values to prevent exponential scales;\n",
    "* Further reduce the data set to only relevant classifiers. \n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create connections to data and read data to pandas dataframe\n",
    "engine = create_engine('sqlite:///../data/orgs_customer_behaviours.db')\n",
    "df_initial_all = pd.read_sql_table('customers_with_behaviours', engine)\n",
    "df_initial_orgs = pd.read_sql_table('orgs_organisations', engine,)"
   ]
  },
  {
   "source": [
    "### Introducing additional data for features and or classifiers do the data set. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ic2019 = pd.read_csv('../data/ic2019.csv', index_col='UNITID')\n",
    "df_adm2019 = pd.read_csv('../data/adm2019.csv', index_col='UNITID')"
   ]
  },
  {
   "source": [
    "### Reduce the files to only the required columns. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ic2019_cls_col = ['PEO1ISTR', 'PEO2ISTR', 'PEO3ISTR', 'PEO4ISTR', 'PEO5ISTR', 'PEO6ISTR', \n",
    "                     'CNTLAFFI', 'PUBPRIME', 'PUBSECON', 'RELAFFIL', 'LEVEL1', 'LEVEL2', \n",
    "                     'LEVEL3', 'LEVEL4', 'LEVEL5', 'LEVEL6', 'LEVEL7', 'LEVEL8', 'LEVEL12',\n",
    "                     'LEVEL17', 'LEVEL18', 'LEVEL19', 'CALSYS', 'FT_UG', 'FT_FTUG', 'FTGDNIDP', \n",
    "                     'PT_UG', 'PT_FTUG', 'PTGDNIDP', 'DOCPP', 'DOCPPSP', 'OPENADMP', 'CREDITS1', \n",
    "                     'CREDITS2', 'CREDITS3', 'CREDITS4', 'STUSRV2', 'STUSRV3', 'STUSRV4', \n",
    "                     'STUSRV8', 'LIBRES1', 'LIBRES2', 'LIBRES3', 'LIBRES4', 'LIBRES5', 'TUITPL', \n",
    "                     'TUITPL1', 'TUITPL2', 'TUITPL3', 'TUITPL4', 'DSTNUGC', 'DSTNUGP', 'DSTNUGN', \n",
    "                     'DSTNGC', 'DSTNGP', 'DSTNGN', 'DISTCRS', 'DISTPGS', 'DSTNCED1', 'DSTNCED2', \n",
    "                     'DSTNCED3', 'DISTNCED', 'DISAB', 'ROOM', 'ROOMCAP', 'BOARD']\n",
    "\n",
    "df_cls_ic2019 = df_ic2019[df_ic2019_cls_col]\n",
    "\n",
    "df_adm2019_cls_cols = ['APPLCN', 'APPLCNM', 'APPLCNW', 'ADMSSN', 'ADMSSNM', 'ADMSSNW', \n",
    "                       'ENRLT', 'ENRLM', 'ENRLW', 'ENRLFT', 'ENRLFTM', 'ENRLFTW', \n",
    "                       'ENRLPT', 'ENRLPTM', 'ENRLPTW', 'SATNUM', 'SATPCT', 'ACTNUM', \n",
    "                       'ACTPCT', 'SATVR25', 'SATVR75', 'SATMT25', 'SATMT75']\n",
    "\n",
    "df_cls_adm2019 = df_adm2019[df_adm2019_cls_cols]"
   ]
  },
  {
   "source": [
    "### Join the data sets together"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_features = df_cls_adm2019.merge(df_cls_ic2019, on='UNITID')\n",
    "df_orgs_data = df_initial_orgs.merge(df_combined_features, on='UNITID')\n",
    "print(f\"\\nThe dataframe df_all_data is shaped with {df_orgs_data.shape[1]} columns and {df_orgs_data.shape[0]} rows\\n\\n\")\n",
    "print(f\"Here is the head of the dataframe ... \\n\\n {df_orgs_data.head()}\")"
   ]
  },
  {
   "source": [
    "#### Here we will remove the `-1` and `-2` placeholders in the data, that was introduced with the data merging. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orgs_data.replace([-2, '-2', -1, '-1'], 0, inplace=True)"
   ]
  },
  {
   "source": [
    "### Remove unnecessary uniqueness columns from the data set, and check for any `NaN` values. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnecessary_uniqueness_columns = ['INSTNM', 'IALIAS', 'FIPS', 'OBEREG', 'GENTELE', 'EIN', 'DUNS', 'OPEID', 'CNGDSTCD']\n",
    "df_orgs_data.drop(unnecessary_uniqueness_columns, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_not_null = df_orgs_data.isnull().sum().sum()\n",
    "col_count = df_orgs_data.shape[1]\n",
    "row_count = df_orgs_data.shape[0]\n",
    "\n",
    "print(f\"The size of the data is {col_count * row_count} with {data_not_null} NaN items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_null = df_orgs_data.isnull().sum().to_frame('NaN')\n",
    "df_data_null[df_data_null['NaN'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_nulls_with_column_median(list_of_columns, dataframe):\n",
    "\n",
    "    \"\"\"\n",
    "    The function takes a list of applicable columns and a corresponding dataframe. \n",
    "    The median is calculated per column and then identifies NaNs to be replaced with the median\n",
    "\n",
    "    INPUT: \n",
    "\n",
    "    list_of_columns: a list of columns with null values\n",
    "    dataframe: the dataframe relating to the specified list.\n",
    "\n",
    "    OUTPUT:\n",
    "\n",
    "    None\n",
    "        The dataframe values are replaced inplace to there is no need to return the frame\n",
    "    \"\"\"\n",
    "\n",
    "    for col in list_of_columns:\n",
    "        impute_median = int(dataframe[col].median())\n",
    "        dataframe[col].fillna(value=impute_median, inplace=True)\n",
    "    \n",
    "    df_dataframe_nulls = dataframe.isnull().sum().to_frame('Nulls')\n",
    "    list_null_cols_vals = df_dataframe_nulls[df_dataframe_nulls['Nulls'] > 0]\n",
    "    \n",
    "    print(f'\\nImputing of values complete, you can find the df header below: \\n\\n {dataframe.head()}')\n",
    "    print(f'\\nLets look for the amount of nulls remaining in the dataframe: \\n\\n')\n",
    "\n",
    "    for rows in list_null_cols_vals.items():\n",
    "        print(f'{rows} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_null_data = list(df_data_null[df_data_null['NaN'] > 0].index)\n",
    "impute_nulls_with_column_median(columns_with_null_data, df_orgs_data)"
   ]
  },
  {
   "source": [
    "#### Now that we've improved the data quality by removing the `NaN` values we need to standardise the values for the KNN model. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = df_orgs_data.select_dtypes(include=['object'])\n",
    "\n",
    "list_of_categorical_columns = list(categorical_columns.columns)\n",
    "\n",
    "list_of_binary_features_columns = ['PEO1ISTR', 'PEO2ISTR', 'PEO3ISTR', 'PEO4ISTR', 'PEO5ISTR', 'PEO6ISTR', 'LEVEL1', 'LEVEL2', 'LEVEL3',\n",
    "                                   'LEVEL4', 'LEVEL5', 'LEVEL6', 'LEVEL7', 'LEVEL8', 'LEVEL12', 'LEVEL17', 'LEVEL18', 'LEVEL19', 'DOCPP', \n",
    "                                   'CREDITS1', 'CREDITS2', 'CREDITS3', 'CREDITS4', 'STUSRV2', 'STUSRV3', 'STUSRV4', 'STUSRV8', 'LIBRES1', \n",
    "                                   'LIBRES2', 'LIBRES3', 'LIBRES4', 'LIBRES5', 'TUITPL1', 'TUITPL2', 'TUITPL3', 'TUITPL4', 'DSTNUGC', \n",
    "                                   'DSTNUGP', 'DSTNUGN', 'DSTNGC', 'DSTNGP', 'DSTNGN', 'DISTCRS', 'DISTPGS', 'DSTNCED1', 'DSTNCED2', \n",
    "                                   'DSTNCED3', 'CONVERTED', 'CONTACTED']\n",
    "\n",
    "list_of_nonbinary_features_columns = ['OPEFLAG', 'SECTOR', 'ICLEVEL', 'CONTROL', 'HLOFFER', 'GROFFER', 'HDEGOFR1', 'DEGGRANT', 'HBCU',\n",
    "                                      'HOSPITAL', 'MEDICAL', 'TRIBAL', 'LOCALE', 'POSTSEC', 'PSEFLAG', 'PSET4FLG', 'RPTMTH', 'INSTCAT',\n",
    "                                      'C18BASIC', 'C18IPUG', 'C18IPGRD', 'C18UGPRF', 'C18ENPRF', 'C18SZSET', 'C15BASIC', 'CCBASIC', \n",
    "                                      'CARNEGIE', 'LANDGRNT', 'INSTSIZE', 'F1SYSTYP', 'F1SYSCOD', 'APPLCN', 'APPLCNM', 'APPLCNW', \n",
    "                                      'ADMSSN', 'ADMSSNM', 'ADMSSNW', 'ENRLT', 'ENRLM', 'ENRLW', 'ENRLFT', 'ENRLFTM', 'ENRLFTW', 'ENRLPT', \n",
    "                                      'ENRLPTM', 'ENRLPTW', 'SATNUM', 'SATPCT', 'ACTNUM', 'ACTPCT', 'SATVR25', 'SATVR75', 'SATMT25', \n",
    "                                      'SATMT75', 'CNTLAFFI', 'PUBPRIME', 'PUBSECON', 'RELAFFIL', 'CALSYS', 'FT_UG', 'FT_FTUG', 'FTGDNIDP', \n",
    "                                      'PT_UG', 'PT_FTUG', 'PTGDNIDP', 'DOCPPSP', 'OPENADMP', 'TUITPL', 'DISTNCED', 'DISAB', 'ROOM', 'BOARD']\n",
    "\n",
    "list_of_unneeded_features_columns = ['DEATHYR', 'CYACTIVE']\n",
    "\n",
    "target_feature = ['CONVERTED']\n",
    "\n",
    "columns_with_only_zero_vals = ['DEATHYR', 'CYACTIVE', 'LEVEL12', 'OPENADMP']"
   ]
  },
  {
   "source": [
    "### The write the data to the sqlite dB"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orgs_data.to_sql('all_data', engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df_orgs_data.drop(df_orgs_data.select_dtypes(include=['object']), axis=1))\n",
    "df.drop(columns=columns_with_only_zero_vals, index=1, inplace=True)\n",
    "df.to_sql('features_target_class', engine, if_exists='replace', index=False)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "source": [
    "## Standardise the data using the `StandardScalar()` from `sklearn`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(df.drop('CONVERTED',axis=1))\n",
    "scaled_features = scaler.transform(df.drop('CONVERTED',axis=1))\n",
    "feature_columns = list(df.columns)\n",
    "feature_columns.remove('UNITID')\n",
    "df_feat = pd.DataFrame(scaled_features, columns=feature_columns)\n",
    "df_feat.to_sql('normalised_features', engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.5)\n",
    "plt.subplots(figsize=(125,100))\n",
    "sns.heatmap(df_feat.corr(), annot=True, linewidths=2, cmap=\"YlGnBu\", fmt=\".2f\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}